{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from string import ascii_letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/moby_dick.txt\"\n",
    "\n",
    "f = open(filename, \"r\", newline='\\n')\n",
    "\n",
    "lines  = f.read().splitlines()\n",
    "full_text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"%7’æ“è;5?)*é(œ.]:â90£!‘8,1/6_&[$”23'4\n"
     ]
    }
   ],
   "source": [
    "# Remove non alphabet characters\n",
    "legal_chars = ascii_letters+ \" \" + \"-\" + \"—\" # apparently there are two different types of -\n",
    "\n",
    "all_chars = list(set(full_text))\n",
    "\n",
    "illegal_chars = [x for x in all_chars if x not in legal_chars]\n",
    "illegal_chars = \"\".join(illegal_chars)\n",
    "\n",
    "print(illegal_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = full_text.translate({ord(x): \"\" for x in illegal_chars})\n",
    "clean_text = clean_text.replace(\"-\", \" \")\n",
    "clean_text = clean_text.replace(\"—\", \" \")\n",
    "\n",
    "clean_text = clean_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter  loomings  call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought i would sail about a little and see the watery part of the world it is a way i have of driving off the spleen and regulating the circulation whenever i find myself growing grim about the mouth whenever it is a damp drizzly november in my soul whenever i find myself involuntarily pausing before coffin warehouses and bringing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = clean_text.split()\n",
    "\n",
    "# Remove two-letter words\n",
    "all_words = [x for x in all_words if 2 < len(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', 'loomings', 'call', 'ishmael', 'some', 'years', 'ago', 'never', 'mind', 'how', 'long', 'precisely', 'having', 'little', 'money', 'purse', 'and', 'nothing', 'particular', 'interest', 'shore', 'thought', 'would', 'sail', 'about', 'little', 'and', 'see', 'the', 'watery', 'part', 'the', 'world', 'way', 'have', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', 'whenever', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', 'whenever', 'damp', 'drizzly', 'november', 'soul', 'whenever', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', 'and', 'bringing', 'the', 'rear', 'every', 'funeral', 'meet', 'and', 'especially', 'whenever', 'hypos', 'get', 'such', 'upper', 'hand', 'that', 'requires', 'strong', 'moral', 'principle', 'prevent', 'from', 'deliberately', 'stepping', 'into', 'the', 'street', 'and', 'methodically', 'knocking', 'peoples', 'hats', 'off', 'then', 'account', 'high']\n"
     ]
    }
   ],
   "source": [
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the counts of every chunk\n",
    "chunk_dict = {}\n",
    "\n",
    "for word in all_words:\n",
    "    for i in range(3, len(word) + 1):\n",
    "        chunk = word[i-3 : i]\n",
    "        count = chunk_dict.get(chunk,0)\n",
    "        chunk_dict[chunk] = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19662, 'the'), (8059, 'ing'), (7848, 'and'), (4037, 'his'), (3873, 'hat'), (3517, 'her'), (3514, 'tha'), (3108, 'ere'), (2925, 'all'), (2739, 'for'), (2564, 'ter'), (2551, 'thi'), (2420, 'wha'), (2247, 'ver'), (2220, 'ent'), (2197, 'ith'), (2059, 'wit'), (2026, 'hal'), (1908, 'but'), (1793, 'ale')]\n"
     ]
    }
   ],
   "source": [
    "# View most common chunks\n",
    "print(sorted(zip(chunk_dict.values(),chunk_dict.keys()),reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'aac'), (1, 'aar'), (1, 'ahu'), (1, 'aia'), (1, 'aic'), (1, 'aja'), (1, 'amt'), (1, 'anf'), (1, 'aot'), (1, 'apu'), (1, 'atb'), (1, 'awr'), (1, 'axl'), (1, 'axo'), (1, 'axy'), (1, 'aya'), (1, 'ayn'), (1, 'ayw'), (1, 'bau'), (1, 'baz')]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(zip(chunk_dict.values(),chunk_dict.keys()),reverse=False)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(chunk_dict.keys())\n",
    "chunk_counts = list(chunk_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wrying'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a word\n",
    "N = 6\n",
    "index = random.randint(0, len(chunks))\n",
    "start = chunks[index]\n",
    "word = start\n",
    "\n",
    "while len(word) < N:\n",
    "    tail = word[-2:]\n",
    "    \n",
    "    potential_words = [x for x in chunks if tail == x[:2]]\n",
    "    probabilities = [chunk_dict[x] for x in potential_words]\n",
    "\n",
    "    chosen_word = random.choices(population=potential_words, weights=probabilities, k=1)[0]\n",
    "    word += chosen_word[-1]\n",
    "\n",
    "word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
